dataset: ogbn-arxiv
# model
input_dim: 4096
hidden_size: 64
layer_num: 1
activation: elu
dropout: 0.2
norm: ln
sampler: khop
encoder: SAGE_Encoder
r: 32
T: 0.1
layer_select: [0,5,10,15,20,25,-1]
# training
lr: 0.0001
weight_decay: 0.005
epochs: 200
earlystop: True
patience: 5
seeds: [25,131,148,404,353]


